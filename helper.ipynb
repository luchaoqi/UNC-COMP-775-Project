{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the high-level, the steps are:\n",
    "\n",
    "1. Run [COLMAP — COLMAP 3.8 documentation](https://colmap.github.io/) to generate the sparse point cloud and the camera poses for custom datasets.\n",
    "\n",
    "2. Run [yenchenlin/nerf-pytorch: A PyTorch implementation of NeRF (Neural Radiance Fields) that reproduces the results.](https://github.com/yenchenlin/nerf-pytorch) to generate the 3D reconstruction.\n",
    "\n",
    "For reproducibility, I demonstrate some of the problems that might occur and the solutions to them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`LLFF` related problems when creating custom data:\n",
    "\n",
    "when converting colmap results (bin files) into the format of LLFF (npy files) using [LLFF/imgs2poses.py at master · Fyusion/LLFF](https://github.com/Fyusion/LLFF/blob/master/imgs2poses.py), the current code repo is having problems and should be solved using code here:\n",
    "\n",
    "[Update pose_utils.py by starhiking · Pull Request #60 · Fyusion/LLFF](https://github.com/Fyusion/LLFF/pull/60)\n",
    "\n",
    "note that due to low quality of images you may also need to delete images according to the output `view_imgs.txt` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`NeRF` related problems:\n",
    "\n",
    "it maybe caused by the compatibility problem when running the `run_nerf.py` file. I solved it by changing the `load_llff.py` file as follows:\n",
    "\n",
    "1. make our own folder e.g. `images_8` (12.5% of original size) where images are rescaled using own code below\n",
    "\n",
    "2. comment out the line `_minify(basedir, factors=[factor])` in `load_llff.py` file - this is to load our own rescaled images in step 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "images = os.listdir('data/nerf_llff_data/colmap/images')\n",
    "\n",
    "export_path = 'data/nerf_llff_data/colmap/images_8'\n",
    "\n",
    "if not os.path.exists(export_path):\n",
    "    os.mkdir(export_path)\n",
    "\n",
    "for image in images:\n",
    "    img = cv2.imread('data/nerf_llff_data/colmap/images/' + image)\n",
    "    scale_percent = 12.5 # percent of original size\n",
    "    width = int(img.shape[1] * scale_percent / 100)\n",
    "    height = int(img.shape[0] * scale_percent / 100)\n",
    "    dim = (width, height)\n",
    "    # resize image\n",
    "    resized = cv2.resize(img, dim)\n",
    "    cv2.imwrite(export_path + '/' + image, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(104, 17)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# load poses_bounds.npy\n",
    "poses_arr = np.load('data/nerf_llff_data/colmap/poses_bounds.npy')\n",
    "poses_arr.shape\n",
    "# here 3 images were dropped due to COLMAP\n",
    "# check `view_imgs.txt` and we can see that the first 3 images are dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.19424518e-04,  9.99985718e-01, -5.34317956e-03,  8.51247692e-02,\n",
       "        2.16000000e+02,  9.99964594e-01,  7.44606569e-05, -8.41459574e-03,\n",
       "       -9.31933576e-01,  2.70000000e+02, -8.41407771e-03, -5.34399529e-03,\n",
       "       -9.99950321e-01, -6.29752099e+00,  1.00959705e+02,  2.91582581e+00,\n",
       "        1.34089396e+01])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poses_arr[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to run NeRF - for better results, use 1x downsampled images (no downsampling in this example)\n",
    "# !python3 run_nerf.py --config configs/colon_paper.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using results from SLAM instead of COLMAP:\n",
    "\n",
    "\n",
    "ref: [Fyusion/LLFF: Code release for Local Light Field Fusion at SIGGRAPH 2019](https://github.com/Fyusion/LLFF#using-your-own-poses-without-running-colmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 12)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pose results\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "path = '/playpen2/luchaoqi/results/031/result/colon_norm_preall_abs_nosm'\n",
    "file = 'pose_result.txt'\n",
    "\n",
    "poses = []\n",
    "with open(os.path.join(path, file), 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            line = line.split(' ')\n",
    "            # remove last element\n",
    "            line.pop()\n",
    "            poses.append(np.array(line, dtype=np.float64))\n",
    "poses = np.array(poses)\n",
    "poses.shape\n",
    "# here from the results we can see 1 image was dropped from SLAM\n",
    "# check the index and we find that the first image was dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.19576538 1.5596371\n"
     ]
    }
   ],
   "source": [
    "# close / far depth\n",
    "path = '/playpen2/luchaoqi/results/031/result/colon_norm_preall_abs_nosm/depth/'\n",
    "mins = []\n",
    "maxs = []\n",
    "import numpy as np\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.bin') and 'pose' not in file:\n",
    "        arr = np.fromfile(os.path.join(path, file), dtype=np.float32)\n",
    "        mins.append(arr.min())\n",
    "        maxs.append(arr.max())\n",
    "print(min(mins), max(maxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(106, 17)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "near = 0.1\n",
    "far = 2.0\n",
    "img_width = 270\n",
    "img_height = 216\n",
    "focal = 145.4410\n",
    "\n",
    "adjusted_poses = []\n",
    "\n",
    "for pose in poses:\n",
    "    # convert pose to 3x4 matrix\n",
    "    pose = pose.reshape(3, 4)\n",
    "    # concatenate each pose with [height, width, focal] to get a 3x5 matrix\n",
    "    pose = np.concatenate((pose, np.array([img_height, img_width, focal]).reshape(3,1)), axis=1)\n",
    "    # flatten the matrix to 15 elements\n",
    "    pose = pose.flatten()\n",
    "    # concatenate near and far depth\n",
    "    pose = np.concatenate((pose, np.array([near, far])))\n",
    "    adjusted_poses.append(np.array(pose, dtype=np.float64))\n",
    "    \n",
    "adjusted_poses = np.array(adjusted_poses)\n",
    "adjusted_poses.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save adjusted poses\n",
    "export_dir = 'data/nerf_llff_data/slam'\n",
    "if not os.path.exists(export_dir):\n",
    "    os.mkdir(export_dir)\n",
    "path = os.path.join(export_dir, 'poses_bounds.npy')\n",
    "np.save(path, adjusted_poses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # resize images\n",
    "\n",
    "# import cv2\n",
    "# import os\n",
    "\n",
    "# images = os.listdir('data/nerf_llff_data/slam/images')\n",
    "\n",
    "# export_path = 'data/nerf_llff_data/slam/images_8'\n",
    "\n",
    "# if not os.path.exists(export_path):\n",
    "#     os.mkdir(export_path)\n",
    "\n",
    "# for image in images:\n",
    "#     img = cv2.imread('data/nerf_llff_data/slam/images/' + image)\n",
    "#     scale_percent = 12.5 # percent of original size\n",
    "#     width = int(img.shape[1] * scale_percent / 100)\n",
    "#     height = int(img.shape[0] * scale_percent / 100)\n",
    "#     dim = (width, height)\n",
    "#     # resize image\n",
    "#     resized = cv2.resize(img, dim)\n",
    "#     cv2.imwrite(export_path + '/' + image, resized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to run NeRF - for better results, use 1x downsampled images (no downsampling in this example)\n",
    "# !python3 run_nerf.py --config configs/slam.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using JHU data. These sequences are more oblique/en face and have relatively more geometric structures for these kinds of views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.16404422 5.5122375\n"
     ]
    }
   ],
   "source": [
    "# close / far depth\n",
    "import os\n",
    "import numpy as np\n",
    "path = 'data/nerf_llff_data/jhu/GeoDepth2/'\n",
    "mins = []\n",
    "maxs = []\n",
    "for file in os.listdir(path):\n",
    "    if file.endswith('.npy'):\n",
    "        # load depth\n",
    "        depth = np.load(os.path.join(path, file))\n",
    "        mins.append(depth.min())\n",
    "        maxs.append(depth.max())\n",
    "print(min(mins), max(maxs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "near = -0.2\n",
    "far = 6.0\n",
    "img_width = 270\n",
    "img_height = 216\n",
    "focal = 154.058\n",
    "\n",
    "path = \"data/nerf_llff_data/jhu\"\n",
    "pose_file = \"pose.txt\"\n",
    "\n",
    "poses = []\n",
    "with open(os.path.join(path, pose_file), 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        if line:\n",
    "            line = line.split(',')\n",
    "            line = line[:-4]\n",
    "            pose = map(float, line)\n",
    "            pose = np.reshape(list(pose), (3, 4))\n",
    "            pose = np.concatenate((pose, np.array([img_height, img_width, focal]).reshape(3,1)), axis=1)\n",
    "            pose = pose.flatten()\n",
    "            pose = np.concatenate((pose, np.array([near, far])))\n",
    "            poses.append(np.array(pose, dtype=np.float64))\n",
    "\n",
    "export_path = os.path.join(path, 'poses_bounds.npy')\n",
    "np.save(export_path, np.array(poses))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ready to run NeRF - for better results, use 1x downsampled images (no downsampling in this example)\n",
    "# !python3 run_nerf.py --config configs/jhu.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
